{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9UBBmUbUmLw",
    "outputId": "ba1e220d-8a21-4b20-9520-6f2dd9926028"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install vocab\n",
    "# !pip install pyvi\n",
    "# !pip install torch_geometric\n",
    "# !pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install torch-scatter\n",
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wCgcoBfNxShE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "import pickle, pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JS5qxJ721Ue9"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=2021):\n",
    "    print(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    }
   ],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_data = 9\n",
    "data_path = f\"../DataPreprocess/Cleaned_Data/{no_label_data}_label/clean_data.csv\"\n",
    "train_data_path = f\"../DataPreprocess/Cleaned_Data/{no_label_data}_label/train_data.csv\"\n",
    "dev_data_path = f\"../DataPreprocess/Cleaned_Data/{no_label_data}_label/dev_data.csv\"\n",
    "test_data_path = f\"../DataPreprocess/Cleaned_Data/{no_label_data}_label/test_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (10888, 12)\n",
      "Valid Set Shape: (3077, 12)\n",
      "Test Set Shape: (1568, 12)\n",
      "Full Set Shape: (15533, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Id_speaker</th>\n",
       "      <th>Utterance_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion_Mutiple</th>\n",
       "      <th>Dialog_id</th>\n",
       "      <th>Label</th>\n",
       "      <th>Utterance_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bao tiền</td>\n",
       "      <td>Nguyễn Thanh Tú</td>\n",
       "      <td>100031059109987</td>\n",
       "      <td>1</td>\n",
       "      <td>18/02/2022</td>\n",
       "      <td>08:07:47</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Bao tiền</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nguyễn Thanh Tú bạn có khum haha</td>\n",
       "      <td>Nguyễn Thị Diễm</td>\n",
       "      <td>100007602498241</td>\n",
       "      <td>2</td>\n",
       "      <td>18/02/2022</td>\n",
       "      <td>08:08:10</td>\n",
       "      <td>Joy</td>\n",
       "      <td>Joy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bạn có khum haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Nguyễn Thị Diễm nổ giá đii đừng ib.</td>\n",
       "      <td>Nguyễn Thanh Tú</td>\n",
       "      <td>100031059109987</td>\n",
       "      <td>3</td>\n",
       "      <td>18/02/2022</td>\n",
       "      <td>08:08:27</td>\n",
       "      <td>Anger</td>\n",
       "      <td>Anger</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>nổ giá đii đừng ib .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>T có nha</td>\n",
       "      <td>Dao Phuong Anh</td>\n",
       "      <td>100009157681703</td>\n",
       "      <td>1</td>\n",
       "      <td>18/02/2022</td>\n",
       "      <td>08:37:06</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>T có nha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dao Phuong Anh check ib ạ</td>\n",
       "      <td>Nguyễn Thị Diễm</td>\n",
       "      <td>100007602498241</td>\n",
       "      <td>2</td>\n",
       "      <td>18/02/2022</td>\n",
       "      <td>08:37:18</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>check ib ạ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Index                            Utterance          Speaker   \n",
       "0     1                             Bao tiền  Nguyễn Thanh Tú  \\\n",
       "1     2     Nguyễn Thanh Tú bạn có khum haha  Nguyễn Thị Diễm   \n",
       "2     3  Nguyễn Thị Diễm nổ giá đii đừng ib.  Nguyễn Thanh Tú   \n",
       "3     4                             T có nha   Dao Phuong Anh   \n",
       "4     5            Dao Phuong Anh check ib ạ  Nguyễn Thị Diễm   \n",
       "\n",
       "        Id_speaker  Utterance_id        Date      Time  Emotion   \n",
       "0  100031059109987             1  18/02/2022  08:07:47  Neutral  \\\n",
       "1  100007602498241             2  18/02/2022  08:08:10      Joy   \n",
       "2  100031059109987             3  18/02/2022  08:08:27    Anger   \n",
       "3  100009157681703             1  18/02/2022  08:37:06  Neutral   \n",
       "4  100007602498241             2  18/02/2022  08:37:18  Neutral   \n",
       "\n",
       "  Emotion_Mutiple  Dialog_id  Label       Utterance_clean  \n",
       "0         Neutral          1      0              Bao tiền  \n",
       "1             Joy          1      1      bạn có khum haha  \n",
       "2           Anger          1      4  nổ giá đii đừng ib .  \n",
       "3         Neutral          2      0              T có nha  \n",
       "4         Neutral          2      0            check ib ạ  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path).fillna(\"\")\n",
    "valid_data = pd.read_csv(dev_data_path).fillna(\"\")\n",
    "test_data = pd.read_csv(test_data_path).fillna(\"\")\n",
    "df = pd.read_csv(data_path).fillna(\"\")\n",
    "\n",
    "print(f\"Train Set Shape: {train_data.shape}\")\n",
    "print(f\"Valid Set Shape: {valid_data.shape}\")\n",
    "print(f\"Test Set Shape: {test_data.shape}\")\n",
    "print(f\"Full Set Shape: {df.shape}\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "frGc92wwx9nh"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UVa3VTTNdsSP"
   },
   "outputs": [],
   "source": [
    "class ERCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path=None, n_classes=9, train=True, dev=True):\n",
    "        self.dialogIDs, self.dialogSpeakers, self.dialogLabels, self.dialogText,\\\n",
    "        self.dialogSentence, self.trainIds, self.testIds, self.devIds = pickle.load(open(path, 'rb'))\n",
    "\n",
    "        '''\n",
    "        label index mapping = {'frustration': 0, 'fear': 1, 'anger': 2, 'disgust': 3, 'sadness' : 4, 'joy': 5, 'surprise': 6, 'excited': 7, 'neutral': 8}\n",
    "        '''\n",
    "\n",
    "        self.keys = []\n",
    "        if train:\n",
    "            for x in self.trainIds:\n",
    "                self.keys.append(x)\n",
    "        elif dev:\n",
    "            for x in self.devIds:\n",
    "                self.keys.append(x)\n",
    "        else:\n",
    "            for x in self.testIds:\n",
    "                self.keys.append(x)   \n",
    "                \n",
    "        self.len = len(self.keys)\n",
    "\n",
    "    def calculate_weight(self):\n",
    "        class_count = Counter()\n",
    "\n",
    "        for vid in self.trainIds:\n",
    "            for uid in self.dialogLabels[vid]:\n",
    "                class_count[uid] += 1\n",
    "\n",
    "        total = sum(class_count.values())\n",
    "        class_weights = {key: total / val for key, val in class_count.items()}\n",
    "        return list(class_weights.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.keys[index]\n",
    "        return torch.FloatTensor(self.dialogText[idx]), \\\n",
    "               torch.FloatTensor(self.dialogSpeakers[idx]), \\\n",
    "               torch.FloatTensor([1] * len(self.dialogLabels[idx])), \\\n",
    "               torch.LongTensor(self.dialogLabels[idx]), \\\n",
    "               idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        dat = pd.DataFrame(data)\n",
    "        return [pad_sequence(dat[i]) if i < 2 else pad_sequence(dat[i], True) if i < 4 else dat[i].tolist() for i in dat]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Md0XgOQq0nG-"
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OTBoKn04WniN"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)\n",
    "            input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = input.gather(1, target).view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHYCeygB0vtc"
   },
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vb12o9R7V-jt"
   },
   "outputs": [],
   "source": [
    "def pad(tensor, length, cuda_flag):\n",
    "    if isinstance(tensor, Variable):\n",
    "        var = tensor\n",
    "        if length > var.size(0):\n",
    "            if cuda_flag:\n",
    "                return torch.cat([var, torch.zeros(length - var.size(0), *var.size()[1:]).cuda()])\n",
    "            else:\n",
    "                return torch.cat([var, torch.zeros(length - var.size(0), *var.size()[1:])])\n",
    "        else:\n",
    "            return var\n",
    "    else:\n",
    "        if length > tensor.size(0):\n",
    "            if cuda_flag:\n",
    "                return torch.cat([tensor, torch.zeros(length - tensor.size(0), *tensor.size()[1:]).cuda()])\n",
    "            else:\n",
    "                return torch.cat([tensor, torch.zeros(length - tensor.size(0), *tensor.size()[1:])])\n",
    "        else:\n",
    "            return tensor\n",
    "\n",
    "\n",
    "def feature_transfer(bank_s_, bank_p_, seq_lengths, cuda_flag=False):\n",
    "    input_conversation_length = torch.tensor(seq_lengths)\n",
    "    start_zero = input_conversation_length.data.new(1).zero_()\n",
    "    if cuda_flag:\n",
    "        input_conversation_length = input_conversation_length.cuda()\n",
    "        start_zero = start_zero.cuda()\n",
    "\n",
    "    max_len = max(seq_lengths)\n",
    "    start = torch.cumsum(torch.cat((start_zero, input_conversation_length[:-1])), 0)\n",
    "    # (l,b,h)\n",
    "    bank_s = torch.stack(\n",
    "        [pad(bank_s_.narrow(0, s, l), max_len, cuda_flag) for s, l in zip(start.data.tolist(), input_conversation_length.data.tolist())], 0\n",
    "    ).transpose(0, 1)\n",
    "    bank_p = torch.stack(\n",
    "        [pad(bank_p_.narrow(0, s, l), max_len, cuda_flag) for s, l in zip(start.data.tolist(), input_conversation_length.data.tolist())], 0\n",
    "    ).transpose(0, 1)\n",
    "\n",
    "    return bank_s, bank_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U7G1D0EMXbxG"
   },
   "outputs": [],
   "source": [
    "class ReasonModule(nn.Module):\n",
    "    def __init__(self, in_channels=200, processing_steps=0, num_layers=1):\n",
    "        \"\"\"\n",
    "        Reasoning Module\n",
    "        \"\"\"\n",
    "        super(ReasonModule, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = 2 * in_channels\n",
    "        self.processing_steps = processing_steps\n",
    "        self.num_layers = num_layers\n",
    "        if processing_steps > 0:\n",
    "            self.lstm = nn.LSTM(self.out_channels, self.in_channels, num_layers)  # 400,200,1\n",
    "            self.lstm.reset_parameters()\n",
    "\n",
    "    def forward(self, x, batch, q_star):\n",
    "        if self.processing_steps <= 0: return q_star\n",
    "\n",
    "        batch_size = batch.max().item() + 1\n",
    "        h = (x.new_zeros((self.num_layers, batch_size, self.in_channels)),\n",
    "             x.new_zeros((self.num_layers, batch_size, self.in_channels)))\n",
    "        for i in range(self.processing_steps):\n",
    "            q, h = self.lstm(q_star.unsqueeze(0), h)\n",
    "            q = q.view(batch_size, self.in_channels)\n",
    "            e = (x * q[batch]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, batch, num_nodes=batch_size)\n",
    "            r = scatter_add(a * x, batch, dim=0, dim_size=batch_size)\n",
    "            q_star = torch.cat([q, r], dim=-1)\n",
    "        return q_star\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w68dxo92XtTS"
   },
   "outputs": [],
   "source": [
    "class CognitionNetwork(nn.Module):\n",
    "    def __init__(self, n_features=200, n_classes=9, dropout=0.2, cuda_flag=False, reason_steps=None):\n",
    "        \"\"\"\n",
    "        Multi-turn Reasoning Modules\n",
    "        \"\"\"\n",
    "        super(CognitionNetwork, self).__init__()\n",
    "        self.cuda_flag = cuda_flag\n",
    "        self.fc = nn.Linear(n_features, n_features * 2)\n",
    "        self.steps = reason_steps if reason_steps is not None else [0, 0]\n",
    "        self.reason_modules = nn.ModuleList([\n",
    "            ReasonModule(in_channels=n_features, processing_steps=self.steps[0], num_layers=1),\n",
    "            ReasonModule(in_channels=n_features, processing_steps=self.steps[1], num_layers=1)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.smax_fc = nn.Linear(n_features * 4, n_classes)\n",
    "\n",
    "    def forward(self, U_s, U_p, seq_lengths):\n",
    "        # (b) <== (l,b,h)\n",
    "        batch_size = U_s.size(1)\n",
    "        batch_index, context_s_, context_p_ = [], [], []\n",
    "        for j in range(batch_size):\n",
    "            batch_index.extend([j] * seq_lengths[j])\n",
    "            context_s_.append(U_s[:seq_lengths[j], j, :])\n",
    "            context_p_.append(U_p[:seq_lengths[j], j, :])\n",
    "\n",
    "        batch_index = torch.tensor(batch_index)\n",
    "        bank_s_ = torch.cat(context_s_, dim=0)\n",
    "        bank_p_ = torch.cat(context_p_, dim=0)\n",
    "        if self.cuda_flag:\n",
    "            batch_index = batch_index.cuda()\n",
    "            bank_s_ = bank_s_.cuda()\n",
    "            bank_p_ = bank_p_.cuda()\n",
    "\n",
    "        # (l,b,h) << (l*b,h)\n",
    "        bank_s, bank_p = feature_transfer(bank_s_, bank_p_, seq_lengths, self.cuda_flag)\n",
    "\n",
    "        feature_ = []\n",
    "        for t in range(bank_s.size(0)):\n",
    "            # (2*h) <== (h)\n",
    "            q_star = self.fc(bank_s[t])\n",
    "            q_situ = self.reason_modules[0](bank_s_, batch_index, q_star)\n",
    "            feature_.append(q_situ.unsqueeze(0))\n",
    "        feature_s = torch.cat(feature_, dim=0)\n",
    "\n",
    "        feature_ = []\n",
    "        for t in range(bank_p.size(0)):\n",
    "            q_star = self.fc(bank_p[t])\n",
    "            q_party = self.reason_modules[1](bank_p_, batch_index, q_star)\n",
    "            feature_.append(q_party.unsqueeze(0))\n",
    "        feature_v = torch.cat(feature_, dim=0)\n",
    "\n",
    "        # (l,b,2*2*h)\n",
    "        hidden = torch.cat([feature_v, feature_s], dim=-1)\n",
    "        hidden = self.dropout(F.relu(hidden))\n",
    "        log_prob = F.log_softmax(self.smax_fc(hidden), 2)\n",
    "        log_prob = torch.cat([log_prob[:, j, :][:seq_lengths[j]] for j in range(len(seq_lengths))])\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MLYT8lIr090N"
   },
   "outputs": [],
   "source": [
    "class DialogueCRN(nn.Module):\n",
    "    def __init__(self, base_model='LSTM', base_layer=2, input_size=None, hidden_size=None, n_speakers=29,\n",
    "                 n_classes=9, dropout=0.2, cuda_flag=False, reason_steps=None):\n",
    "        \"\"\"\n",
    "        Contextual Reasoning Network\n",
    "        \"\"\"\n",
    "\n",
    "        super(DialogueCRN, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.n_speakers = n_speakers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.base_model == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=base_layer, bidirectional=True, dropout=dropout)\n",
    "            self.rnn_parties = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=base_layer, bidirectional=True, dropout=dropout)\n",
    "        elif self.base_model == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=base_layer, bidirectional=True, dropout=dropout)\n",
    "            self.rnn_parties = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=base_layer, bidirectional=True, dropout=dropout)\n",
    "        elif self.base_model == 'Linear':\n",
    "            self.base_linear = nn.Linear(input_size, 2 * hidden_size)\n",
    "        else:\n",
    "            print('Base model must be one of LSTM/GRU/Linear')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.cognition_net = CognitionNetwork(n_features=2 * hidden_size, n_classes=n_classes, dropout=dropout, cuda_flag=cuda_flag, reason_steps=reason_steps)\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, U, qmask, seq_lengths):\n",
    "        U_s, U_p = None, None\n",
    "\n",
    "        if self.base_model == 'LSTM':\n",
    "            # (b,l,h), (b,l,p)\n",
    "            U_, qmask_ = U.transpose(0, 1), qmask.transpose(0, 1)\n",
    "            U_p_ = torch.zeros(U_.size()[0], U_.size()[1], self.hidden_size * 2).type(U.type())\n",
    "            U_parties_ = [torch.zeros_like(U_).type(U_.type()) for _ in range(self.n_speakers)]\n",
    "            for b in range(U_.size(0)):\n",
    "                for p in range(len(U_parties_)):\n",
    "                    index_i = torch.nonzero(qmask_[b][:, p]).squeeze(-1)\n",
    "                    if index_i.size(0) > 0:\n",
    "                        U_parties_[p][b][:index_i.size(0)] = U_[b][index_i]\n",
    "            E_parties_ = [self.rnn_parties(U_parties_[p].transpose(0, 1))[0].transpose(0, 1) for p in range(len(U_parties_))]\n",
    "\n",
    "            for b in range(U_p_.size(0)):\n",
    "                for p in range(len(U_parties_)):\n",
    "                    index_i = torch.nonzero(qmask_[b][:, p]).squeeze(-1)\n",
    "                    if index_i.size(0) > 0: U_p_[b][index_i] = E_parties_[p][b][:index_i.size(0)]\n",
    "            U_p = U_p_.transpose(0, 1)\n",
    "\n",
    "            # (l,b,2*h) [(2*bi,b,h) * 2]\n",
    "            U_s, hidden = self.rnn(U)\n",
    "\n",
    "        elif self.base_model == 'GRU':\n",
    "            U_, qmask_ = U.transpose(0, 1), qmask.transpose(0, 1)\n",
    "            U_p_ = torch.zeros(U_.size()[0], U_.size()[1], self.hidden_size * 2).type(U.type())\n",
    "            U_parties_ = [torch.zeros_like(U_).type(U_.type()) for _ in range(self.n_speakers)]  # default 2\n",
    "            for b in range(U_.size(0)):\n",
    "                for p in range(len(U_parties_)):\n",
    "                    index_i = torch.nonzero(qmask_[b][:, p]).squeeze(-1)\n",
    "                    if index_i.size(0) > 0: U_parties_[p][b][:index_i.size(0)] = U_[b][index_i]\n",
    "            E_parties_ = [self.rnn_parties(U_parties_[p].transpose(0, 1))[0].transpose(0, 1) for p in range(len(U_parties_))]\n",
    "\n",
    "            for b in range(U_p_.size(0)):\n",
    "                for p in range(len(U_parties_)):\n",
    "                    index_i = torch.nonzero(qmask_[b][:, p]).squeeze(-1)\n",
    "                    if index_i.size(0) > 0: U_p_[b][index_i] = E_parties_[p][b][:index_i.size(0)]\n",
    "            U_p = U_p_.transpose(0, 1)\n",
    "            U_s, hidden = self.rnn(U)\n",
    "        elif self.base_model == 'Linear':\n",
    "            # TODO\n",
    "            U = self.base_linear(U)\n",
    "            U = self.dropout(F.relu(U))\n",
    "            hidden = self.smax_fc(U)\n",
    "            log_prob = F.log_softmax(hidden, 2)\n",
    "            logits = torch.cat([log_prob[:, j, :][:seq_lengths[j]] for j in range(len(seq_lengths))])\n",
    "            return logits\n",
    "\n",
    "        logits = self.cognition_net(U_s, U_p, seq_lengths)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iluuwGN1OKk"
   },
   "source": [
    "#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UVsbY68d-1dQ"
   },
   "outputs": [],
   "source": [
    "# def get_train_valid_sampler(trainset, valid=0.1):\n",
    "#     size = len(trainset)\n",
    "#     idx = list(range(size))\n",
    "#     split = int(valid * size)\n",
    "#     return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CuXXMsUJ-1gO"
   },
   "outputs": [],
   "source": [
    "def get_ERC_loaders(path, n_classes, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
    "    trainset = ERCDataset(path=path, n_classes=n_classes, train=True, dev=False)\n",
    "    devset = ERCDataset(path=path, n_classes=n_classes, train=False, dev=True)\n",
    "    testset = ERCDataset(path=path, n_classes=n_classes, train=False, dev=False)\n",
    "    \n",
    "    train_loader = DataLoader(trainset,\n",
    "                              batch_size=batch_size,\n",
    "                              collate_fn=trainset.collate_fn,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "    valid_loader = DataLoader(devset,\n",
    "                              batch_size=batch_size,\n",
    "                              collate_fn=trainset.collate_fn,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "\n",
    "    test_loader = DataLoader(testset,\n",
    "                             batch_size=batch_size,\n",
    "                             collate_fn=testset.collate_fn,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpwothj2-1i_",
    "outputId": "a31d7dc6-7641-46c4-c98a-62cc1e1677d8"
   },
   "outputs": [],
   "source": [
    "# get_ERC_loaders(\"ERC_dataset_clean.pkl\", 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hHh5xwbi-1lu"
   },
   "outputs": [],
   "source": [
    "def train_or_eval_model(model, loss_f, dataloader, epoch=0, train_flag=False, optimizer=None, cuda_flag=False, target_names=None,\n",
    "                        tensorboard=False):\n",
    "    assert not train_flag or optimizer != None\n",
    "    losses, preds, labels, masks = [], [], [], []\n",
    "\n",
    "    if train_flag:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for data in dataloader:\n",
    "        if train_flag: optimizer.zero_grad()\n",
    "\n",
    "        textf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda_flag else data[:-1]\n",
    "        seq_lengths = [(umask[j] == 1).nonzero().tolist()[-1][0] + 1 for j in range(len(umask))]\n",
    "\n",
    "        dataf = textf \n",
    "\n",
    "        log_prob = model(dataf, qmask, seq_lengths)\n",
    "        label = torch.cat([label[j][:seq_lengths[j]] for j in range(len(label))])\n",
    "        umask = torch.cat([umask[j][:seq_lengths[j]] for j in range(len(umask))])\n",
    "        loss = loss_f(log_prob, label)\n",
    "\n",
    "        preds.append(torch.argmax(log_prob, 1).data.cpu().numpy())\n",
    "        labels.append(label.data.cpu().numpy())\n",
    "        masks.append(umask.view(-1).cpu().numpy())\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if train_flag:\n",
    "            loss.backward()\n",
    "            if tensorboard:\n",
    "                for param in model.named_parameters():\n",
    "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
    "            optimizer.step()\n",
    "\n",
    "    if preds != []:\n",
    "        preds = np.concatenate(preds)\n",
    "        labels = np.concatenate(labels)\n",
    "    else:\n",
    "        return float('nan'), float('nan'), float('nan'), [], []\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    avg_loss = round(np.sum(losses) / len(losses), 4)\n",
    "    avg_accuracy = round(accuracy_score(labels, preds) * 100, 2)\n",
    "    avg_fscore = round(f1_score(labels, preds, average='weighted') * 100, 2)\n",
    "\n",
    "    all_matrix = []\n",
    "    all_matrix.append(\n",
    "        metrics.classification_report(labels, preds, target_names=target_names if target_names else None, digits=4))\n",
    "    all_matrix.append([\"ACC\"])\n",
    "    for i in range(len(target_names)):\n",
    "        all_matrix[-1].append(\"{}: {:.4f}\".format(target_names[i], accuracy_score(labels[labels == i], preds[labels == i])))\n",
    "\n",
    "    return avg_loss, avg_accuracy, avg_fscore, all_matrix, []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irvSl3RkC4j4"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5fz_rzk_jwf",
    "outputId": "8c8256cd-a71a-44d3-aa96-cfaa2dfaab40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_layer=1, base_model='LSTM', batch_size=32, class_weight=False, cls_type='emotion', data_dir='Data_test_9_2.pkl', dropout=0.2, epochs=100, gamma=1, l2=0.0002, load_model_state_dir='Model_Result/DialogueCRN/dialoguecrn_22.pkl', lr=0.0001, no_cuda=False, output_dir='Model_Result/DialogueCRN', patience=20, seed=2021, status='train', step_p=0, step_s=2, tensorboard=False)\n",
      "2021\n",
      "DialogueCRN(\n",
      "  (rnn): LSTM(1024, 100, dropout=0.2, bidirectional=True)\n",
      "  (rnn_parties): LSTM(1024, 100, dropout=0.2, bidirectional=True)\n",
      "  (cognition_net): CognitionNetwork(\n",
      "    (fc): Linear(in_features=200, out_features=400, bias=True)\n",
      "    (reason_modules): ModuleList(\n",
      "      (0): ReasonModule(200, 400)\n",
      "      (1): ReasonModule(200, 400)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (smax_fc): Linear(in_features=800, out_features=9, bias=True)\n",
      "  )\n",
      ")\n",
      "Running on GPU\n",
      "DialogueCRN with LSTM as base model.\n",
      "The model have 2370809 paramerters in total\n",
      "epoch: 0, train_loss: 1.2261, train_acc: 54.77, train_fscore: 47.07, valid_loss: 1.1475, valid_acc: 47.94, valid_fscore: 44.84, test_loss: 1.1436, test_acc: 48.41, test_fscore: 45.3, time: 28.36 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.4990    0.6103    0.5490       390\n",
      " Frustration     0.5210    0.6998    0.5973       443\n",
      "     Sadness     0.4362    0.2425    0.3118       268\n",
      "     Neutral     0.6207    0.2628    0.3692       137\n",
      "     Disgust     0.0000    0.0000    0.0000        32\n",
      "     Excited     0.3860    0.1964    0.2604       112\n",
      "         Joy     0.0000    0.0000    0.0000        28\n",
      "    Surprise     0.3793    0.5986    0.4644       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.4841      1568\n",
      "   macro avg     0.3158    0.2900    0.2836      1568\n",
      "weighted avg     0.4632    0.4841    0.4530      1568\n",
      "\n",
      "['ACC', 'Fear: 0.6103', 'Frustration: 0.6998', 'Sadness: 0.2425', 'Neutral: 0.2628', 'Disgust: 0.0000', 'Excited: 0.1964', 'Joy: 0.0000', 'Surprise: 0.5986', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 1, train_loss: 0.4993, train_acc: 78.58, train_fscore: 76.82, valid_loss: 1.1885, valid_acc: 50.18, valid_fscore: 49.28, test_loss: 1.1792, test_acc: 50.06, test_fscore: 49.04, time: 28.26 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5152    0.5667    0.5397       390\n",
      " Frustration     0.5475    0.6501    0.5944       443\n",
      "     Sadness     0.4038    0.3134    0.3529       268\n",
      "     Neutral     0.6111    0.4818    0.5388       137\n",
      "     Disgust     0.6667    0.1250    0.2105        32\n",
      "     Excited     0.3590    0.3750    0.3668       112\n",
      "         Joy     0.8571    0.2143    0.3429        28\n",
      "    Surprise     0.4431    0.5034    0.4713       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5006      1568\n",
      "   macro avg     0.4893    0.3589    0.3797      1568\n",
      "weighted avg     0.5013    0.5006    0.4904      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5667', 'Frustration: 0.6501', 'Sadness: 0.3134', 'Neutral: 0.4818', 'Disgust: 0.1250', 'Excited: 0.3750', 'Joy: 0.2143', 'Surprise: 0.5034', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 2, train_loss: 0.3981, train_acc: 81.97, train_fscore: 81.43, valid_loss: 1.197, valid_acc: 51.28, valid_fscore: 50.8, test_loss: 1.1888, test_acc: 50.51, test_fscore: 49.82, time: 27.39 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5252    0.5615    0.5428       390\n",
      " Frustration     0.5543    0.6456    0.5965       443\n",
      "     Sadness     0.3982    0.3284    0.3599       268\n",
      "     Neutral     0.6415    0.4964    0.5597       137\n",
      "     Disgust     0.4615    0.1875    0.2667        32\n",
      "     Excited     0.3621    0.3750    0.3684       112\n",
      "         Joy     0.6500    0.4643    0.5417        28\n",
      "    Surprise     0.4403    0.4762    0.4575       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5051      1568\n",
      "   macro avg     0.4481    0.3928    0.4103      1568\n",
      "weighted avg     0.4995    0.5051    0.4982      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5615', 'Frustration: 0.6456', 'Sadness: 0.3284', 'Neutral: 0.4964', 'Disgust: 0.1875', 'Excited: 0.3750', 'Joy: 0.4643', 'Surprise: 0.4762', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 3, train_loss: 0.3641, train_acc: 83.25, train_fscore: 82.89, valid_loss: 1.1996, valid_acc: 51.41, valid_fscore: 50.94, test_loss: 1.1912, test_acc: 50.32, test_fscore: 49.67, time: 27.13 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5229    0.5564    0.5391       390\n",
      " Frustration     0.5521    0.6456    0.5952       443\n",
      "     Sadness     0.3899    0.3172    0.3498       268\n",
      "     Neutral     0.6415    0.4964    0.5597       137\n",
      "     Disgust     0.4444    0.2500    0.3200        32\n",
      "     Excited     0.3590    0.3750    0.3668       112\n",
      "         Joy     0.6667    0.5000    0.5714        28\n",
      "    Surprise     0.4452    0.4694    0.4570       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5032      1568\n",
      "   macro avg     0.4469    0.4011    0.4177      1568\n",
      "weighted avg     0.4971    0.5032    0.4967      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5564', 'Frustration: 0.6456', 'Sadness: 0.3172', 'Neutral: 0.4964', 'Disgust: 0.2500', 'Excited: 0.3750', 'Joy: 0.5000', 'Surprise: 0.4694', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 4, train_loss: 0.3464, train_acc: 84.06, train_fscore: 83.73, valid_loss: 1.2033, valid_acc: 51.67, valid_fscore: 51.2, test_loss: 1.1938, test_acc: 50.06, test_fscore: 49.43, time: 27.81 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5207    0.5487    0.5343       390\n",
      " Frustration     0.5511    0.6456    0.5946       443\n",
      "     Sadness     0.3937    0.3246    0.3558       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.4211    0.2500    0.3137        32\n",
      "     Excited     0.3590    0.3750    0.3668       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4444    0.4626    0.4533       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5006      1568\n",
      "   macro avg     0.4363    0.3987    0.4126      1568\n",
      "weighted avg     0.4942    0.5006    0.4943      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5487', 'Frustration: 0.6456', 'Sadness: 0.3246', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3750', 'Joy: 0.5000', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 5, train_loss: 0.3328, train_acc: 84.26, train_fscore: 83.91, valid_loss: 1.2053, valid_acc: 51.8, valid_fscore: 51.28, test_loss: 1.1938, test_acc: 50.06, test_fscore: 49.42, time: 29.41 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5167    0.5564    0.5358       390\n",
      " Frustration     0.5536    0.6411    0.5941       443\n",
      "     Sadness     0.3909    0.3209    0.3525       268\n",
      "     Neutral     0.6321    0.4891    0.5514       137\n",
      "     Disgust     0.3810    0.2500    0.3019        32\n",
      "     Excited     0.3596    0.3661    0.3628       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4503    0.4626    0.4564       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5006      1568\n",
      "   macro avg     0.4325    0.3985    0.4116      1568\n",
      "weighted avg     0.4935    0.5006    0.4942      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5564', 'Frustration: 0.6411', 'Sadness: 0.3209', 'Neutral: 0.4891', 'Disgust: 0.2500', 'Excited: 0.3661', 'Joy: 0.5000', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 6, train_loss: 0.3222, train_acc: 85.01, train_fscore: 84.7, valid_loss: 1.2056, valid_acc: 51.84, valid_fscore: 51.4, test_loss: 1.1912, test_acc: 49.74, test_fscore: 49.21, time: 29.15 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5154    0.5564    0.5351       390\n",
      " Frustration     0.5594    0.6275    0.5915       443\n",
      "     Sadness     0.3810    0.3284    0.3527       268\n",
      "     Neutral     0.6321    0.4891    0.5514       137\n",
      "     Disgust     0.3333    0.2188    0.2642        32\n",
      "     Excited     0.3475    0.3661    0.3565       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4503    0.4626    0.4564       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.4974      1568\n",
      "   macro avg     0.4253    0.3943    0.4063      1568\n",
      "weighted avg     0.4913    0.4974    0.4921      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5564', 'Frustration: 0.6275', 'Sadness: 0.3284', 'Neutral: 0.4891', 'Disgust: 0.2188', 'Excited: 0.3661', 'Joy: 0.5000', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, train_loss: 0.3109, train_acc: 85.28, train_fscore: 84.99, valid_loss: 1.2084, valid_acc: 51.87, valid_fscore: 51.54, test_loss: 1.1914, test_acc: 50.13, test_fscore: 49.67, time: 27.94 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5154    0.5590    0.5363       390\n",
      " Frustration     0.5688    0.6253    0.5957       443\n",
      "     Sadness     0.3917    0.3507    0.3701       268\n",
      "     Neutral     0.6321    0.4891    0.5514       137\n",
      "     Disgust     0.3182    0.2188    0.2593        32\n",
      "     Excited     0.3504    0.3661    0.3581       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4533    0.4626    0.4579       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5013      1568\n",
      "   macro avg     0.4265    0.3968    0.4086      1568\n",
      "weighted avg     0.4959    0.5013    0.4967      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5590', 'Frustration: 0.6253', 'Sadness: 0.3507', 'Neutral: 0.4891', 'Disgust: 0.2188', 'Excited: 0.3661', 'Joy: 0.5000', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 8, train_loss: 0.3022, train_acc: 85.51, train_fscore: 85.23, valid_loss: 1.2139, valid_acc: 51.9, valid_fscore: 51.51, test_loss: 1.1944, test_acc: 50.32, test_fscore: 49.79, time: 27.74 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5103    0.5692    0.5382       390\n",
      " Frustration     0.5738    0.6321    0.6015       443\n",
      "     Sadness     0.3991    0.3470    0.3713       268\n",
      "     Neutral     0.6250    0.4745    0.5394       137\n",
      "     Disgust     0.3043    0.2188    0.2545        32\n",
      "     Excited     0.3540    0.3571    0.3556       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4564    0.4626    0.4595       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5032      1568\n",
      "   macro avg     0.4257    0.3957    0.4077      1568\n",
      "weighted avg     0.4970    0.5032    0.4979      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5692', 'Frustration: 0.6321', 'Sadness: 0.3470', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3571', 'Joy: 0.5000', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 9, train_loss: 0.2917, train_acc: 85.86, train_fscore: 85.6, valid_loss: 1.2182, valid_acc: 52.62, valid_fscore: 52.39, test_loss: 1.1977, test_acc: 50.7, test_fscore: 50.31, time: 27.53 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5176    0.5641    0.5399       390\n",
      " Frustration     0.5795    0.6253    0.6015       443\n",
      "     Sadness     0.4032    0.3806    0.3916       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.3043    0.2188    0.2545        32\n",
      "     Excited     0.3509    0.3571    0.3540       112\n",
      "         Joy     0.6087    0.5000    0.5490        28\n",
      "    Surprise     0.4694    0.4694    0.4694       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5070      1568\n",
      "   macro avg     0.4291    0.3997    0.4117      1568\n",
      "weighted avg     0.5025    0.5070    0.5031      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5641', 'Frustration: 0.6253', 'Sadness: 0.3806', 'Neutral: 0.4818', 'Disgust: 0.2188', 'Excited: 0.3571', 'Joy: 0.5000', 'Surprise: 0.4694', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 10, train_loss: 0.2823, train_acc: 86.29, train_fscore: 86.03, valid_loss: 1.2245, valid_acc: 52.65, valid_fscore: 52.49, test_loss: 1.2037, test_acc: 51.21, test_fscore: 50.87, time: 27.44 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5198    0.5718    0.5446       390\n",
      " Frustration     0.5927    0.6208    0.6064       443\n",
      "     Sadness     0.4075    0.4030    0.4053       268\n",
      "     Neutral     0.6226    0.4818    0.5432       137\n",
      "     Disgust     0.3043    0.2188    0.2545        32\n",
      "     Excited     0.3611    0.3482    0.3545       112\n",
      "         Joy     0.6250    0.5357    0.5769        28\n",
      "    Surprise     0.4698    0.4762    0.4730       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5121      1568\n",
      "   macro avg     0.4337    0.4062    0.4176      1568\n",
      "weighted avg     0.5080    0.5121    0.5087      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5718', 'Frustration: 0.6208', 'Sadness: 0.4030', 'Neutral: 0.4818', 'Disgust: 0.2188', 'Excited: 0.3482', 'Joy: 0.5357', 'Surprise: 0.4762', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 11, train_loss: 0.2725, train_acc: 86.57, train_fscore: 86.34, valid_loss: 1.2312, valid_acc: 52.68, valid_fscore: 52.51, test_loss: 1.208, test_acc: 51.4, test_fscore: 51.06, time: 28.64 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5199    0.5692    0.5435       390\n",
      " Frustration     0.5940    0.6275    0.6103       443\n",
      "     Sadness     0.4189    0.4142    0.4165       268\n",
      "     Neutral     0.6168    0.4818    0.5410       137\n",
      "     Disgust     0.3043    0.2188    0.2545        32\n",
      "     Excited     0.3482    0.3482    0.3482       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4755    0.4626    0.4690       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5140      1568\n",
      "   macro avg     0.4367    0.4064    0.4190      1568\n",
      "weighted avg     0.5099    0.5140    0.5106      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5692', 'Frustration: 0.6275', 'Sadness: 0.4142', 'Neutral: 0.4818', 'Disgust: 0.2188', 'Excited: 0.3482', 'Joy: 0.5357', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 12, train_loss: 0.262, train_acc: 87.02, train_fscore: 86.8, valid_loss: 1.2428, valid_acc: 53.04, valid_fscore: 52.85, test_loss: 1.2192, test_acc: 51.59, test_fscore: 51.25, time: 30.6 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5231    0.5795    0.5499       390\n",
      " Frustration     0.5953    0.6275    0.6110       443\n",
      "     Sadness     0.4211    0.4179    0.4195       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.3333    0.2500    0.2857        32\n",
      "     Excited     0.3364    0.3304    0.3333       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4752    0.4558    0.4653       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5159      1568\n",
      "   macro avg     0.4406    0.4087    0.4220      1568\n",
      "weighted avg     0.5122    0.5159    0.5125      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5795', 'Frustration: 0.6275', 'Sadness: 0.4179', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3304', 'Joy: 0.5357', 'Surprise: 0.4558', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 13, train_loss: 0.2537, train_acc: 87.48, train_fscore: 87.26, valid_loss: 1.257, valid_acc: 52.94, valid_fscore: 52.83, test_loss: 1.2327, test_acc: 52.36, test_fscore: 52.08, time: 29.12 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5294    0.5769    0.5521       390\n",
      " Frustration     0.6065    0.6298    0.6179       443\n",
      "     Sadness     0.4353    0.4515    0.4432       268\n",
      "     Neutral     0.6346    0.4818    0.5477       137\n",
      "     Disgust     0.3200    0.2500    0.2807        32\n",
      "     Excited     0.3364    0.3304    0.3333       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4895    0.4762    0.4828       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5236      1568\n",
      "   macro avg     0.4449    0.4147    0.4273      1568\n",
      "weighted avg     0.5210    0.5236    0.5208      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5769', 'Frustration: 0.6298', 'Sadness: 0.4515', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3304', 'Joy: 0.5357', 'Surprise: 0.4762', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 14, train_loss: 0.2445, train_acc: 87.72, train_fscore: 87.5, valid_loss: 1.2683, valid_acc: 53.04, valid_fscore: 52.99, test_loss: 1.2403, test_acc: 52.42, test_fscore: 52.22, time: 28.56 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5220    0.5769    0.5481       390\n",
      " Frustration     0.6185    0.6185    0.6185       443\n",
      "     Sadness     0.4390    0.4701    0.4541       268\n",
      "     Neutral     0.6346    0.4818    0.5477       137\n",
      "     Disgust     0.3478    0.2500    0.2909        32\n",
      "     Excited     0.3274    0.3304    0.3289       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4931    0.4830    0.4880       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5242      1568\n",
      "   macro avg     0.4483    0.4163    0.4294      1568\n",
      "weighted avg     0.5234    0.5242    0.5222      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5769', 'Frustration: 0.6185', 'Sadness: 0.4701', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3304', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, train_loss: 0.2354, train_acc: 88.15, train_fscore: 87.95, valid_loss: 1.2844, valid_acc: 53.14, valid_fscore: 53.09, test_loss: 1.2543, test_acc: 52.81, test_fscore: 52.58, time: 29.31 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5268    0.5795    0.5519       390\n",
      " Frustration     0.6130    0.6185    0.6157       443\n",
      "     Sadness     0.4555    0.4776    0.4663       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.3333    0.2500    0.2857        32\n",
      "     Excited     0.3333    0.3393    0.3363       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5034    0.4966    0.5000       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5281      1568\n",
      "   macro avg     0.4496    0.4199    0.4322      1568\n",
      "weighted avg     0.5264    0.5281    0.5258      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5795', 'Frustration: 0.6185', 'Sadness: 0.4776', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4966', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 16, train_loss: 0.2246, train_acc: 88.61, train_fscore: 88.43, valid_loss: 1.3011, valid_acc: 53.14, valid_fscore: 53.09, test_loss: 1.271, test_acc: 52.68, test_fscore: 52.5, time: 29.58 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5319    0.5769    0.5535       390\n",
      " Frustration     0.6154    0.6140    0.6147       443\n",
      "     Sadness     0.4471    0.4888    0.4670       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.3478    0.2500    0.2909        32\n",
      "     Excited     0.3190    0.3304    0.3246       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5035    0.4898    0.4966       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5268      1568\n",
      "   macro avg     0.4495    0.4186    0.4312      1568\n",
      "weighted avg     0.5262    0.5268    0.5250      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5769', 'Frustration: 0.6140', 'Sadness: 0.4888', 'Neutral: 0.4818', 'Disgust: 0.2500', 'Excited: 0.3304', 'Joy: 0.5357', 'Surprise: 0.4898', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 17, train_loss: 0.2159, train_acc: 88.85, train_fscore: 88.67, valid_loss: 1.316, valid_acc: 52.97, valid_fscore: 52.82, test_loss: 1.2886, test_acc: 52.36, test_fscore: 51.99, time: 29.51 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5283    0.5744    0.5504       390\n",
      " Frustration     0.5954    0.6411    0.6174       443\n",
      "     Sadness     0.4574    0.4403    0.4487       268\n",
      "     Neutral     0.6311    0.4745    0.5417       137\n",
      "     Disgust     0.3182    0.2188    0.2593        32\n",
      "     Excited     0.3103    0.3214    0.3158       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4966    0.4898    0.4932       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5236      1568\n",
      "   macro avg     0.4433    0.4107    0.4238      1568\n",
      "weighted avg     0.5198    0.5236    0.5199      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5744', 'Frustration: 0.6411', 'Sadness: 0.4403', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3214', 'Joy: 0.5357', 'Surprise: 0.4898', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 18, train_loss: 0.2073, train_acc: 89.17, train_fscore: 88.97, valid_loss: 1.3393, valid_acc: 53.27, valid_fscore: 53.32, test_loss: 1.309, test_acc: 52.87, test_fscore: 52.79, time: 29.5 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5357    0.5769    0.5556       390\n",
      " Frustration     0.6370    0.5982    0.6170       443\n",
      "     Sadness     0.4369    0.5299    0.4789       268\n",
      "     Neutral     0.6346    0.4818    0.5477       137\n",
      "     Disgust     0.3182    0.2188    0.2593        32\n",
      "     Excited     0.3276    0.3393    0.3333       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5000    0.4830    0.4913       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5287      1568\n",
      "   macro avg     0.4491    0.4182    0.4302      1568\n",
      "weighted avg     0.5318    0.5287    0.5279      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5769', 'Frustration: 0.5982', 'Sadness: 0.5299', 'Neutral: 0.4818', 'Disgust: 0.2188', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 19, train_loss: 0.198, train_acc: 89.68, train_fscore: 89.51, valid_loss: 1.3586, valid_acc: 53.23, valid_fscore: 53.32, test_loss: 1.3227, test_acc: 52.49, test_fscore: 52.4, time: 30.73 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5341    0.5821    0.5571       390\n",
      " Frustration     0.6335    0.5892    0.6105       443\n",
      "     Sadness     0.4361    0.5224    0.4754       268\n",
      "     Neutral     0.6190    0.4745    0.5372       137\n",
      "     Disgust     0.3333    0.2188    0.2642        32\n",
      "     Excited     0.3162    0.3304    0.3231       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4931    0.4830    0.4880       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5249      1568\n",
      "   macro avg     0.4464    0.4151    0.4271      1568\n",
      "weighted avg     0.5277    0.5249    0.5240      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5821', 'Frustration: 0.5892', 'Sadness: 0.5224', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3304', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 20, train_loss: 0.1871, train_acc: 90.0, train_fscore: 89.82, valid_loss: 1.3944, valid_acc: 52.84, valid_fscore: 53.0, test_loss: 1.3577, test_acc: 51.91, test_fscore: 51.98, time: 31.12 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5335    0.5718    0.5520       390\n",
      " Frustration     0.6480    0.5485    0.5941       443\n",
      "     Sadness     0.4160    0.5634    0.4786       268\n",
      "     Neutral     0.6250    0.4745    0.5394       137\n",
      "     Disgust     0.3333    0.2188    0.2642        32\n",
      "     Excited     0.3223    0.3482    0.3348       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4965    0.4830    0.4897       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5191      1568\n",
      "   macro avg     0.4474    0.4160    0.4268      1568\n",
      "weighted avg     0.5295    0.5191    0.5198      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5718', 'Frustration: 0.5485', 'Sadness: 0.5634', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3482', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 21, train_loss: 0.1769, train_acc: 90.57, train_fscore: 90.41, valid_loss: 1.4027, valid_acc: 53.04, valid_fscore: 53.15, test_loss: 1.3646, test_acc: 52.61, test_fscore: 52.53, time: 28.73 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5296    0.5744    0.5510       390\n",
      " Frustration     0.6247    0.5937    0.6088       443\n",
      "     Sadness     0.4539    0.5149    0.4825       268\n",
      "     Neutral     0.6262    0.4891    0.5492       137\n",
      "     Disgust     0.3333    0.2188    0.2642        32\n",
      "     Excited     0.3175    0.3571    0.3361       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4965    0.4830    0.4897       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5261      1568\n",
      "   macro avg     0.4482    0.4185    0.4300      1568\n",
      "weighted avg     0.5282    0.5261    0.5253      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5744', 'Frustration: 0.5937', 'Sadness: 0.5149', 'Neutral: 0.4891', 'Disgust: 0.2188', 'Excited: 0.3571', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 22, train_loss: 0.1701, train_acc: 90.74, train_fscore: 90.58, valid_loss: 1.4238, valid_acc: 53.07, valid_fscore: 52.96, test_loss: 1.3907, test_acc: 52.1, test_fscore: 51.71, time: 27.64 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5251    0.5897    0.5556       390\n",
      " Frustration     0.5932    0.6321    0.6120       443\n",
      "     Sadness     0.4545    0.4104    0.4314       268\n",
      "     Neutral     0.6168    0.4818    0.5410       137\n",
      "     Disgust     0.3182    0.2188    0.2593        32\n",
      "     Excited     0.3115    0.3393    0.3248       112\n",
      "         Joy     0.6250    0.5357    0.5769        28\n",
      "    Surprise     0.5035    0.4830    0.4931       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5210      1568\n",
      "   macro avg     0.4387    0.4101    0.4216      1568\n",
      "weighted avg     0.5169    0.5210    0.5171      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5897', 'Frustration: 0.6321', 'Sadness: 0.4104', 'Neutral: 0.4818', 'Disgust: 0.2188', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4830', 'Anger: 0.0000']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, train_loss: 0.1669, train_acc: 90.86, train_fscore: 90.71, valid_loss: 1.4526, valid_acc: 52.58, valid_fscore: 52.58, test_loss: 1.4213, test_acc: 52.81, test_fscore: 52.55, time: 28.55 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5293    0.5795    0.5532       390\n",
      " Frustration     0.6115    0.6253    0.6183       443\n",
      "     Sadness     0.4539    0.4776    0.4655       268\n",
      "     Neutral     0.6286    0.4818    0.5455       137\n",
      "     Disgust     0.3158    0.1875    0.2353        32\n",
      "     Excited     0.3306    0.3661    0.3475       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5111    0.4694    0.4894       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5281      1568\n",
      "   macro avg     0.4481    0.4136    0.4270      1568\n",
      "weighted avg     0.5265    0.5281    0.5255      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5795', 'Frustration: 0.6253', 'Sadness: 0.4776', 'Neutral: 0.4818', 'Disgust: 0.1875', 'Excited: 0.3661', 'Joy: 0.5357', 'Surprise: 0.4694', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 24, train_loss: 0.1592, train_acc: 91.19, train_fscore: 91.05, valid_loss: 1.4924, valid_acc: 52.36, valid_fscore: 52.57, test_loss: 1.4572, test_acc: 52.23, test_fscore: 52.35, time: 27.33 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5308    0.5744    0.5517       390\n",
      " Frustration     0.6622    0.5530    0.6027       443\n",
      "     Sadness     0.4211    0.5672    0.4833       268\n",
      "     Neutral     0.6262    0.4891    0.5492       137\n",
      "     Disgust     0.3182    0.2188    0.2593        32\n",
      "     Excited     0.3203    0.3661    0.3417       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5037    0.4626    0.4823       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5223      1568\n",
      "   macro avg     0.4483    0.4185    0.4287      1568\n",
      "weighted avg     0.5340    0.5223    0.5235      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5744', 'Frustration: 0.5530', 'Sadness: 0.5672', 'Neutral: 0.4891', 'Disgust: 0.2188', 'Excited: 0.3661', 'Joy: 0.5357', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 25, train_loss: 0.1546, train_acc: 91.32, train_fscore: 91.19, valid_loss: 1.5121, valid_acc: 52.19, valid_fscore: 52.4, test_loss: 1.4757, test_acc: 51.47, test_fscore: 51.61, time: 27.1 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5266    0.5590    0.5423       390\n",
      " Frustration     0.6457    0.5553    0.5971       443\n",
      "     Sadness     0.4016    0.5485    0.4637       268\n",
      "     Neutral     0.6311    0.4745    0.5417       137\n",
      "     Disgust     0.3500    0.2188    0.2692        32\n",
      "     Excited     0.3228    0.3661    0.3431       112\n",
      "         Joy     0.7143    0.5357    0.6122        28\n",
      "    Surprise     0.5000    0.4626    0.4806       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5147      1568\n",
      "   macro avg     0.4547    0.4134    0.4278      1568\n",
      "weighted avg     0.5270    0.5147    0.5161      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5590', 'Frustration: 0.5553', 'Sadness: 0.5485', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3661', 'Joy: 0.5357', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 26, train_loss: 0.1484, train_acc: 91.62, train_fscore: 91.51, valid_loss: 1.5265, valid_acc: 52.62, valid_fscore: 52.77, test_loss: 1.4881, test_acc: 51.85, test_fscore: 51.93, time: 27.04 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5238    0.5641    0.5432       390\n",
      " Frustration     0.6604    0.5530    0.6020       443\n",
      "     Sadness     0.4059    0.5634    0.4719       268\n",
      "     Neutral     0.6190    0.4745    0.5372       137\n",
      "     Disgust     0.3333    0.1875    0.2400        32\n",
      "     Excited     0.3417    0.3661    0.3534       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5036    0.4762    0.4895       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5185      1568\n",
      "   macro avg     0.4489    0.4134    0.4250      1568\n",
      "weighted avg     0.5304    0.5185    0.5193      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5641', 'Frustration: 0.5530', 'Sadness: 0.5634', 'Neutral: 0.4745', 'Disgust: 0.1875', 'Excited: 0.3661', 'Joy: 0.5357', 'Surprise: 0.4762', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 27, train_loss: 0.1526, train_acc: 91.17, train_fscore: 91.06, valid_loss: 1.5304, valid_acc: 52.29, valid_fscore: 52.43, test_loss: 1.4925, test_acc: 51.66, test_fscore: 51.72, time: 27.22 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5190    0.5590    0.5383       390\n",
      " Frustration     0.6495    0.5688    0.6065       443\n",
      "     Sadness     0.4105    0.5560    0.4723       268\n",
      "     Neutral     0.6250    0.4745    0.5394       137\n",
      "     Disgust     0.3500    0.2188    0.2692        32\n",
      "     Excited     0.3248    0.3393    0.3319       112\n",
      "         Joy     0.6818    0.5357    0.6000        28\n",
      "    Surprise     0.4962    0.4490    0.4714       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5166      1568\n",
      "   macro avg     0.4508    0.4112    0.4254      1568\n",
      "weighted avg     0.5264    0.5166    0.5172      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5590', 'Frustration: 0.5688', 'Sadness: 0.5560', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4490', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 28, train_loss: 0.1454, train_acc: 91.72, train_fscore: 91.64, valid_loss: 1.5252, valid_acc: 52.58, valid_fscore: 52.39, test_loss: 1.4886, test_acc: 52.23, test_fscore: 51.84, time: 27.11 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5253    0.5846    0.5534       390\n",
      " Frustration     0.6055    0.6411    0.6228       443\n",
      "     Sadness     0.4409    0.4590    0.4497       268\n",
      "     Neutral     0.6154    0.4672    0.5311       137\n",
      "     Disgust     0.3684    0.2188    0.2745        32\n",
      "     Excited     0.3063    0.3036    0.3049       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5000    0.4354    0.4655       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5223      1568\n",
      "   macro avg     0.4460    0.4050    0.4211      1568\n",
      "weighted avg     0.5188    0.5223    0.5184      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5846', 'Frustration: 0.6411', 'Sadness: 0.4590', 'Neutral: 0.4672', 'Disgust: 0.2188', 'Excited: 0.3036', 'Joy: 0.5357', 'Surprise: 0.4354', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 29, train_loss: 0.1379, train_acc: 92.15, train_fscore: 92.07, valid_loss: 1.5475, valid_acc: 52.45, valid_fscore: 52.09, test_loss: 1.5127, test_acc: 52.87, test_fscore: 52.23, time: 27.23 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5218    0.5821    0.5503       390\n",
      " Frustration     0.5906    0.6772    0.6309       443\n",
      "     Sadness     0.4689    0.4216    0.4440       268\n",
      "     Neutral     0.6190    0.4745    0.5372       137\n",
      "     Disgust     0.3333    0.1875    0.2400        32\n",
      "     Excited     0.3455    0.3393    0.3423       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5118    0.4422    0.4745       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5287      1568\n",
      "   macro avg     0.4492    0.4067    0.4230      1568\n",
      "weighted avg     0.5220    0.5287    0.5223      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5821', 'Frustration: 0.6772', 'Sadness: 0.4216', 'Neutral: 0.4745', 'Disgust: 0.1875', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4422', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 30, train_loss: 0.1296, train_acc: 92.49, train_fscore: 92.41, valid_loss: 1.6034, valid_acc: 52.49, valid_fscore: 51.9, test_loss: 1.5701, test_acc: 51.47, test_fscore: 50.49, time: 27.34 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.4967    0.5872    0.5382       390\n",
      " Frustration     0.5744    0.6885    0.6263       443\n",
      "     Sadness     0.4554    0.3619    0.4033       268\n",
      "     Neutral     0.6200    0.4526    0.5232       137\n",
      "     Disgust     0.3125    0.1562    0.2083        32\n",
      "     Excited     0.3200    0.2857    0.3019       112\n",
      "         Joy     0.6818    0.5357    0.6000        28\n",
      "    Surprise     0.5000    0.4218    0.4576       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5147      1568\n",
      "   macro avg     0.4401    0.3877    0.4065      1568\n",
      "weighted avg     0.5061    0.5147    0.5049      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5872', 'Frustration: 0.6885', 'Sadness: 0.3619', 'Neutral: 0.4526', 'Disgust: 0.1562', 'Excited: 0.2857', 'Joy: 0.5357', 'Surprise: 0.4218', 'Anger: 0.0000']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, train_loss: 0.1239, train_acc: 92.88, train_fscore: 92.82, valid_loss: 1.6382, valid_acc: 52.16, valid_fscore: 51.49, test_loss: 1.6096, test_acc: 52.04, test_fscore: 51.17, time: 29.28 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5022    0.5897    0.5425       390\n",
      " Frustration     0.5758    0.6862    0.6262       443\n",
      "     Sadness     0.4587    0.3731    0.4115       268\n",
      "     Neutral     0.6465    0.4672    0.5424       137\n",
      "     Disgust     0.3529    0.1875    0.2449        32\n",
      "     Excited     0.3366    0.3036    0.3192       112\n",
      "         Joy     0.6818    0.5357    0.6000        28\n",
      "    Surprise     0.5081    0.4286    0.4649       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5204      1568\n",
      "   macro avg     0.4514    0.3968    0.4168      1568\n",
      "weighted avg     0.5135    0.5204    0.5117      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5897', 'Frustration: 0.6862', 'Sadness: 0.3731', 'Neutral: 0.4672', 'Disgust: 0.1875', 'Excited: 0.3036', 'Joy: 0.5357', 'Surprise: 0.4286', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 32, train_loss: 0.1144, train_acc: 93.5, train_fscore: 93.44, valid_loss: 1.6384, valid_acc: 52.32, valid_fscore: 52.07, test_loss: 1.6104, test_acc: 51.85, test_fscore: 51.27, time: 27.11 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5089    0.5872    0.5452       390\n",
      " Frustration     0.5818    0.6501    0.6141       443\n",
      "     Sadness     0.4496    0.3993    0.4229       268\n",
      "     Neutral     0.6262    0.4891    0.5492       137\n",
      "     Disgust     0.3529    0.1875    0.2449        32\n",
      "     Excited     0.3214    0.3214    0.3214       112\n",
      "         Joy     0.6818    0.5357    0.6000        28\n",
      "    Surprise     0.5159    0.4422    0.4762       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5185      1568\n",
      "   macro avg     0.4487    0.4014    0.4193      1568\n",
      "weighted avg     0.5132    0.5185    0.5127      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5872', 'Frustration: 0.6501', 'Sadness: 0.3993', 'Neutral: 0.4891', 'Disgust: 0.1875', 'Excited: 0.3214', 'Joy: 0.5357', 'Surprise: 0.4422', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 33, train_loss: 0.1088, train_acc: 93.6, train_fscore: 93.53, valid_loss: 1.6654, valid_acc: 52.23, valid_fscore: 52.15, test_loss: 1.6195, test_acc: 52.61, test_fscore: 52.13, time: 27.23 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5206    0.5821    0.5496       390\n",
      " Frustration     0.5992    0.6546    0.6257       443\n",
      "     Sadness     0.4545    0.4291    0.4415       268\n",
      "     Neutral     0.6075    0.4745    0.5328       137\n",
      "     Disgust     0.3889    0.2188    0.2800        32\n",
      "     Excited     0.3393    0.3393    0.3393       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5075    0.4626    0.4840       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5261      1568\n",
      "   macro avg     0.4522    0.4107    0.4268      1568\n",
      "weighted avg     0.5209    0.5261    0.5213      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5821', 'Frustration: 0.6546', 'Sadness: 0.4291', 'Neutral: 0.4745', 'Disgust: 0.2188', 'Excited: 0.3393', 'Joy: 0.5357', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 34, train_loss: 0.1033, train_acc: 94.15, train_fscore: 94.11, valid_loss: 1.6733, valid_acc: 52.55, valid_fscore: 52.54, test_loss: 1.6347, test_acc: 52.87, test_fscore: 52.54, time: 27.23 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5253    0.5846    0.5534       390\n",
      " Frustration     0.6129    0.6433    0.6278       443\n",
      "     Sadness     0.4656    0.4291    0.4466       268\n",
      "     Neutral     0.6036    0.4891    0.5403       137\n",
      "     Disgust     0.3333    0.2188    0.2642        32\n",
      "     Excited     0.3307    0.3750    0.3515       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5036    0.4762    0.4895       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5287      1568\n",
      "   macro avg     0.4475    0.4169    0.4290      1568\n",
      "weighted avg     0.5254    0.5287    0.5254      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5846', 'Frustration: 0.6433', 'Sadness: 0.4291', 'Neutral: 0.4891', 'Disgust: 0.2188', 'Excited: 0.3750', 'Joy: 0.5357', 'Surprise: 0.4762', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 35, train_loss: 0.0985, train_acc: 94.15, train_fscore: 94.1, valid_loss: 1.7054, valid_acc: 52.49, valid_fscore: 52.59, test_loss: 1.655, test_acc: 52.04, test_fscore: 51.95, time: 27.33 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5301    0.5872    0.5572       390\n",
      " Frustration     0.6291    0.6050    0.6168       443\n",
      "     Sadness     0.4413    0.4627    0.4517       268\n",
      "     Neutral     0.6038    0.4672    0.5267       137\n",
      "     Disgust     0.3000    0.1875    0.2308        32\n",
      "     Excited     0.3037    0.3661    0.3320       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4792    0.4694    0.4742       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5204      1568\n",
      "   macro avg     0.4377    0.4090    0.4197      1568\n",
      "weighted avg     0.5221    0.5204    0.5195      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5872', 'Frustration: 0.6050', 'Sadness: 0.4627', 'Neutral: 0.4672', 'Disgust: 0.1875', 'Excited: 0.3661', 'Joy: 0.5357', 'Surprise: 0.4694', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 36, train_loss: 0.0935, train_acc: 94.65, train_fscore: 94.62, valid_loss: 1.7457, valid_acc: 52.23, valid_fscore: 52.24, test_loss: 1.7119, test_acc: 52.68, test_fscore: 52.41, time: 27.36 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5239    0.5897    0.5549       390\n",
      " Frustration     0.6096    0.6275    0.6185       443\n",
      "     Sadness     0.4699    0.4366    0.4526       268\n",
      "     Neutral     0.6071    0.4964    0.5462       137\n",
      "     Disgust     0.3500    0.2188    0.2692        32\n",
      "     Excited     0.3182    0.3750    0.3443       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.5074    0.4694    0.4876       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5268      1568\n",
      "   macro avg     0.4487    0.4166    0.4291      1568\n",
      "weighted avg     0.5250    0.5268    0.5241      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5897', 'Frustration: 0.6275', 'Sadness: 0.4366', 'Neutral: 0.4964', 'Disgust: 0.2188', 'Excited: 0.3750', 'Joy: 0.5357', 'Surprise: 0.4694', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 37, train_loss: 0.0918, train_acc: 94.51, train_fscore: 94.46, valid_loss: 1.7793, valid_acc: 52.06, valid_fscore: 52.3, test_loss: 1.7261, test_acc: 51.34, test_fscore: 51.51, time: 27.6 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5140    0.5641    0.5379       390\n",
      " Frustration     0.6578    0.5598    0.6049       443\n",
      "     Sadness     0.4217    0.5224    0.4667       268\n",
      "     Neutral     0.6204    0.4891    0.5469       137\n",
      "     Disgust     0.2778    0.1562    0.2000        32\n",
      "     Excited     0.2917    0.3750    0.3281       112\n",
      "         Joy     0.6522    0.5357    0.5882        28\n",
      "    Surprise     0.4964    0.4626    0.4789       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5134      1568\n",
      "   macro avg     0.4369    0.4072    0.4168      1568\n",
      "weighted avg     0.5247    0.5134    0.5151      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5641', 'Frustration: 0.5598', 'Sadness: 0.5224', 'Neutral: 0.4891', 'Disgust: 0.1562', 'Excited: 0.3750', 'Joy: 0.5357', 'Surprise: 0.4626', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "epoch: 38, train_loss: 0.0899, train_acc: 94.66, train_fscore: 94.63, valid_loss: 1.817, valid_acc: 51.54, valid_fscore: 51.76, test_loss: 1.7613, test_acc: 51.28, test_fscore: 51.51, time: 27.35 sec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fear     0.5287    0.5897    0.5576       390\n",
      " Frustration     0.6902    0.5079    0.5852       443\n",
      "     Sadness     0.4067    0.5858    0.4801       268\n",
      "     Neutral     0.6226    0.4818    0.5432       137\n",
      "     Disgust     0.2941    0.1562    0.2041        32\n",
      "     Excited     0.2878    0.3571    0.3187       112\n",
      "         Joy     0.6818    0.5357    0.6000        28\n",
      "    Surprise     0.4889    0.4490    0.4681       147\n",
      "       Anger     0.0000    0.0000    0.0000        11\n",
      "\n",
      "    accuracy                         0.5128      1568\n",
      "   macro avg     0.4445    0.4070    0.4174      1568\n",
      "weighted avg     0.5350    0.5128    0.5151      1568\n",
      "\n",
      "['ACC', 'Fear: 0.5897', 'Frustration: 0.5079', 'Sadness: 0.5858', 'Neutral: 0.4818', 'Disgust: 0.1562', 'Excited: 0.3571', 'Joy: 0.5357', 'Surprise: 0.4490', 'Anger: 0.0000']\n",
      "\n",
      "\n",
      "Early stoping... 20 38\n",
      "Final Test performance...\n",
      "Early stoping... 20 38\n",
      "Eval-metric: F1, Epoch: 18, best_eval_fscore: 53.32, Accuracy: 52.87, F1-Score: 52.79\n",
      "Eval-metric: Loss, Epoch: 0, Accuracy: 48.41, F1-Score: 45.3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--status', type=str, default='train', help='optional status: train/test')\n",
    "\n",
    "    parser.add_argument('--data_dir', type=str, default=f'Data_test_{no_label_data}.pkl', help='dataset dir')\n",
    "\n",
    "    parser.add_argument('--output_dir', type=str, default='Model_Result/DialogueCRN', help='saved model dir')\n",
    "\n",
    "    parser.add_argument('--load_model_state_dir', type=str, default='Model_Result/DialogueCRN/dialoguecrn_22.pkl', help='load model state dir')\n",
    "\n",
    "    parser.add_argument('--base_model', default='LSTM', help='base model, LSTM/GRU/Linear')\n",
    "\n",
    "    parser.add_argument('--base_layer', type=int, default=1, help='the number of base model layers, 1/2')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='E', help='number of epochs')\n",
    "\n",
    "    parser.add_argument('--patience', type=int, default=20, help='early stop')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
    "\n",
    "#     parser.add_argument('--valid_rate', type=float, default=0, metavar='valid_rate', help='valid rate: 0.0/0.1')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
    "\n",
    "    parser.add_argument('--l2', type=float, default=0.0002, metavar='L2', help='L2 regularization weight')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0.2, metavar='dropout', help='dropout rate')\n",
    "\n",
    "    parser.add_argument('--step_s', type=int, default=2, help='the number of reason turns at situation-level,3')\n",
    "\n",
    "    parser.add_argument('--step_p', type=int, default=0, help='the number of reason turns at speaker-level,0')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=1, help='gamma 0/0.5/1/2')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
    "\n",
    "    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weights')\n",
    "\n",
    "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
    "\n",
    "    parser.add_argument('--cls_type', type=str, default='emotion', help='choose between sentiment or emotion')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(args)\n",
    "\n",
    "    epochs, batch_size, status, output_path, data_path, load_model_state_dir, base_model, base_layer  = \\\n",
    "        args.epochs, args.batch_size, args.status, args.output_dir, args.data_dir, args.load_model_state_dir, args.base_model, args.base_layer\n",
    "    cuda_flag = torch.cuda.is_available() and not args.no_cuda\n",
    "    reason_steps = [args.step_s, args.step_p]\n",
    "\n",
    "    if args.tensorboard:\n",
    "        from tensorboardX import SummaryWriter\n",
    "\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "    # ERC dataset PhoBert large - embedding\n",
    "    n_speakers, hidden_size, input_size = 29, 100, 1024\n",
    "    n_classes = no_label_data\n",
    "    target_names = list(set(df['Emotion']))\n",
    "    class_weights = torch.FloatTensor(\n",
    "            ERCDataset(data_path).calculate_weight())\n",
    "\n",
    "    seed_everything(seed=args.seed)\n",
    "    model = DialogueCRN(base_model=base_model,\n",
    "                        base_layer=base_layer,\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        n_speakers=n_speakers,\n",
    "                        n_classes=n_classes,\n",
    "                        dropout=args.dropout,\n",
    "                        cuda_flag=cuda_flag,\n",
    "                        reason_steps=reason_steps)\n",
    "\n",
    "    if cuda_flag:\n",
    "        print('Running on GPU')\n",
    "        # torch.cuda.set_device(0)\n",
    "        class_weights = class_weights.cuda()\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('Running on CPU')\n",
    "\n",
    "    name = 'DialogueCRN'\n",
    "    print('{} with {} as base model.'.format(name, base_model))\n",
    "    print(\"The model have {} paramerters in total\".format(sum(x.numel() for x in model.parameters())))\n",
    "\n",
    "    loss_f = FocalLoss(gamma=args.gamma, alpha=class_weights if args.class_weight else None)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    train_loader, valid_loader, test_loader = get_ERC_loaders(data_path,\n",
    "                                                               n_classes,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               num_workers=0)\n",
    "\n",
    "    if status == 'train':\n",
    "        all_test_fscore, all_test_acc = [], []\n",
    "        best_epoch, best_epoch2, patience, best_eval_fscore, best_eval_loss = -1, -1, 0, 0, None\n",
    "        patience2 = 0\n",
    "        for e in range(epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc, train_fscore, _, _ = train_or_eval_model(model=model, loss_f=loss_f, dataloader=train_loader, train_flag=True,\n",
    "                                                                            optimizer=optimizer, cuda_flag=cuda_flag,\n",
    "                                                                            target_names=target_names)\n",
    "            valid_loss, valid_acc, valid_fscore, _, _ = train_or_eval_model(model=model, loss_f=loss_f, dataloader=valid_loader,\n",
    "                                                                            cuda_flag=cuda_flag, target_names=target_names)\n",
    "            test_loss, test_acc, test_fscore, test_metrics, _ = train_or_eval_model(model=model, loss_f=loss_f, dataloader=test_loader,\n",
    "                                                                                    cuda_flag=cuda_flag, target_names=target_names)\n",
    "            all_test_fscore.append(test_fscore)\n",
    "            all_test_acc.append(test_acc)\n",
    "\n",
    "\n",
    "            eval_loss, _, eval_fscore = valid_loss, valid_acc, valid_fscore\n",
    "            \n",
    "            if e == 0 or best_eval_fscore < eval_fscore:\n",
    "                patience = 0\n",
    "                best_epoch, best_eval_fscore = e, eval_fscore\n",
    "                if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "                save_model_dir = os.path.join(output_path, '{}_{}.pkl'.format(name, e).lower())\n",
    "                torch.save(model.state_dict(), save_model_dir)\n",
    "            else:\n",
    "                patience += 1\n",
    "            if best_eval_loss is None:\n",
    "                best_eval_loss = eval_loss\n",
    "                best_epoch2 = 0\n",
    "            else:\n",
    "                if eval_loss < best_eval_loss:\n",
    "                    best_epoch2, best_eval_loss = e, eval_loss\n",
    "                    patience2 = 0\n",
    "                    if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "                    save_model_dir = os.path.join(output_path, 'loss_{}_{}.pkl'.format(name, e).lower())\n",
    "                    torch.save(model.state_dict(), save_model_dir)\n",
    "\n",
    "                else:\n",
    "                    patience2 += 1\n",
    "\n",
    "            if args.tensorboard:\n",
    "                writer.add_scalar('train: accuracy/f1/loss', train_acc / train_fscore / train_loss, e)\n",
    "                writer.add_scalar('valid: accuracy/f1/loss', valid_acc / valid_fscore / valid_loss, e)\n",
    "                writer.add_scalar('test: accuracy/f1/loss', test_acc / test_fscore / test_loss, e)\n",
    "                writer.close()\n",
    "\n",
    "            print(\n",
    "                'epoch: {}, train_loss: {}, train_acc: {}, train_fscore: {}, valid_loss: {}, valid_acc: {}, valid_fscore: {}, test_loss: {}, test_acc: {}, test_fscore: {}, time: {} sec'. \\\n",
    "                    format(e, train_loss, train_acc, train_fscore, valid_loss, valid_acc, valid_fscore, test_loss, test_acc, test_fscore,\n",
    "                           round(time.time() - start_time, 2)))\n",
    "            print(test_metrics[0])\n",
    "            print(test_metrics[1])\n",
    "            print('\\n')\n",
    "\n",
    "            if patience >= args.patience and patience2 >= args.patience:\n",
    "                print('Early stoping...', patience, patience2)\n",
    "                break\n",
    "\n",
    "        print('Final Test performance...')\n",
    "        print('Early stoping...', patience, patience2)\n",
    "        print('Eval-metric: F1, Epoch: {}, best_eval_fscore: {}, Accuracy: {}, F1-Score: {}'.format(best_epoch, best_eval_fscore,\n",
    "                                                                                                    all_test_acc[best_epoch] if best_epoch >= 0 else 0,\n",
    "                                                                                                    all_test_fscore[best_epoch] if best_epoch >= 0 else 0))\n",
    "        print('Eval-metric: Loss, Epoch: {}, Accuracy: {}, F1-Score: {}'.format(best_epoch2,\n",
    "                                                                                all_test_acc[best_epoch2] if best_epoch2 >= 0 else 0,\n",
    "                                                                                all_test_fscore[best_epoch2] if best_epoch2 >= 0 else 0))\n",
    "\n",
    "    elif status == 'test':\n",
    "        start_time = time.time()\n",
    "        model.load_state_dict(torch.load(args.load_model_state_dir))\n",
    "        test_loss, test_acc, test_fscore, test_metrics, test_outputs = train_or_eval_model(model=model, loss_f=loss_f, dataloader=test_loader,\n",
    "                                                                                           cuda_flag=cuda_flag,\n",
    "                                                                                           target_names=target_names)\n",
    "\n",
    "        if args.tensorboard:\n",
    "            writer.add_scalar('test: accuracy/loss', test_acc / test_loss, 0)\n",
    "            writer.close()\n",
    "        print('test_loss: {}, test_acc: {}, test_fscore: {}, time: {} sec'.format(test_loss, test_acc, test_fscore, round(time.time() - start_time, 2)))\n",
    "        print(test_metrics[0])\n",
    "        print(test_metrics[1])\n",
    "    else:\n",
    "        print('the status must be one of train/test')\n",
    "        exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
